{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirection LSTM - IMDB sentiment classification\n",
    "\n",
    "see **https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py**\n",
    "\n",
    "see **https://github.com/transcranial/keras-js/blob/master/notebooks/demos/imdb_bidirectional_lstm.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KERAS_MODEL_FILEPATH = 'imdb_bidirectional_lstm.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare our Data\n",
    "\n",
    "max_features - How many words from our dataset do we want to use? The top 20,000 most common.\n",
    "\n",
    "maxlen - We're going to pad or truncate all the reviews after 200 words.\n",
    "\n",
    "imdb.load_data - We're pulling from a 'toy' dataset and using 20,000 of the to words. The words are already in a tokenized format. (eg. 10 = the, 11=movie, 171=great)\n",
    "\n",
    "Create a 50/50 split of test and train data\n",
    "\n",
    "we now need to pad our sequences to 200 words. Keras pads from the beginning of the senquence.\n",
    "\n",
    "Now, we have a 2 dimensional array size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 200)\n",
      "X_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Model Architecture\n",
    "\n",
    "We're creating a \"Sequential\" model. Think of this as a for loop or do-while with a single exit point. Beginning to end.\n",
    "\n",
    "We're using a \"Word Embedding\" layer. Allow me to explain...\n",
    "\n",
    "Next layer is an Bidirectional LSTM. Remember, LTSMs are \"smart\" neurons that can remember the events that preceeded them. This helps provide context to the use of words. \"I did hate\" and \"I didn't hate\" are very different sentiments but both have the word \"Hate\" in them. Without taking the previous word into consideration, the sentiment will be wrong. This is a 32 neuron wide layer. Wider layers can be smarter. But, deeper architectures provide far more value. Unless you specify, the default \"Activation\" function is Tanh.\n",
    "\n",
    "We're using a .5 dropout. This seems a little high to me, but its possible this dataset overfits quickly.\n",
    "\n",
    "Lastly, we're adding a dense layer. There's a direct relationship between number of neurons and number of \"Things\" you're looking for. In our case, we're finding a binary output, 0 or 1. We only want an activation function to activate on 1, so anything lower than .50 will be considered 0, thus only having 1 neuron.\n",
    "\n",
    "Finally, we compile our model. We're using the \"Adam\" optimizer with the \"Binary Crossentropy\" loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "The \"Checkpointer\" is a method of saving your trained model periodically\n",
    "\n",
    "Early Stopping prevents you from over training your model beyond the point of diminishing returns.\n",
    "\n",
    "Batch size, tweak this value a bit. You don't want a batch size too large, but too small and training will take forever!\n",
    "\n",
    "Epochs, again, this is how many times you want your model to look at all your data.\n",
    "\n",
    "model.fit is where we begin training our data.\n",
    "\n",
    "    x_train - our 25,000 training datapoints\n",
    "    y_train - our \"ground truth\" labels. (the back of the flash card)\n",
    "    \n",
    "    callbacks - Tell the trainer to occasionally update your variables defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.7829Epoch 00001: val_acc improved from -inf to 0.87052, saving model to imdb_bidirectional_lstm.h5\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.4606 - acc: 0.7830 - val_loss: 0.3138 - val_acc: 0.8705\n",
      "Epoch 2/10\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9181Epoch 00002: val_acc did not improve\n",
      "25000/25000 [==============================] - 126s 5ms/step - loss: 0.2279 - acc: 0.9181 - val_loss: 0.3349 - val_acc: 0.8697\n",
      "Epoch 3/10\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9471Epoch 00003: val_acc did not improve\n",
      "25000/25000 [==============================] - 132s 5ms/step - loss: 0.1598 - acc: 0.9472 - val_loss: 0.3556 - val_acc: 0.8661\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f421566c9e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model saving callback\n",
    "checkpointer = ModelCheckpoint(filepath=KERAS_MODEL_FILEPATH, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', verbose=1, patience=2)\n",
    "\n",
    "# train\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=[X_test, y_test],\n",
    "          batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "          callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try out some models\n",
    "\n",
    "**https://transcranial.github.io/keras-js/#/imdb-bidirectional-lstm**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
